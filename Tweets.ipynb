{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1cf80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from autocorrect import Speller\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d2095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "speller = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a43420",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/aleksandrmorozov/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandrmorozov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandrmorozov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksandrmorozov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/aleksandrmorozov/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee8686",
   "metadata": {},
   "source": [
    "## Введение в NLP\n",
    "\n",
    "Одной из наиболее распространенных задач обработки естественного языка является классификация текста в соответствии с его смыслом. В данном случае необходимо правильно распознать тональность текста: негативную, нейтральную и позитивную.\n",
    "\n",
    "### Предобработка данных\n",
    "\n",
    "Для решения поставленной задачи необходимо правильно обработать данные для того, чтобы их можно было использовать для обучения.\n",
    "Сперва проводится очистка данных. Этот шаг может включать в себя:\n",
    "* Токенизацию текста - разбивку предложений на отдельные слова\n",
    "* Обработку ошибок - исправление синтаксических ошибок в словах\n",
    "* Удаление символов, не являющихся буквенными/цифренными\n",
    "* Удаление стоп-слов - удаление слов, не носящих смысловой нагрузки (предлоги, местоимения, междометия и т.п.)\n",
    "* Лемматизацию - преведение различных форм слова к словарной форме\n",
    "* Стемминг - сокращение слов до грамматических основ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb0db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    \n",
    "    def __init__(self, data_path='data', handle_stopwords=False, handle_misspell=False):\n",
    "        \n",
    "        tweets = pd.DataFrame()\n",
    "        \n",
    "        files = {\n",
    "            'processedNegative.csv': -1,\n",
    "            'processedNeutral.csv': 0,\n",
    "            'processedPositive.csv': 1\n",
    "        }\n",
    "        \n",
    "        for file, val in files.items():\n",
    "            with open(f'{data_path}/{file}') as f:\n",
    "                data = f.read()\n",
    "            data = re.sub(r',(?=[^ ])', '--split-here--', data)\n",
    "            tmp = pd.DataFrame(data.split('--split-here--'), columns=['tweet'])\n",
    "            tmp['target'] = val\n",
    "            tmp.drop_duplicates(inplace=True)\n",
    "            tweets = pd.concat([tweets, tmp])\n",
    "        \n",
    "        tweets.tweet = tweets.tweet.str.replace(r'[^a-z A-Z]+', '', regex=True)\n",
    "        self.tweets = tweets.dropna().reset_index(drop=True)\n",
    "        self.stopwords = stopwords.words('english')\n",
    "        self.handle_stopwords = handle_stopwords\n",
    "        self.handle_misspell = handle_misspell\n",
    "        self.__tokenize()\n",
    "\n",
    "    \n",
    "    def reduce_length(self, word):\n",
    "        \n",
    "        pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "        return pattern.sub(r\"\\1\\1\", word)\n",
    "    \n",
    "    \n",
    "    def misspell_handler(self, token_arr):\n",
    "        \n",
    "        speller = Speller(lang='en')\n",
    "        corrected = []\n",
    "        \n",
    "        for sent in token_arr:\n",
    "            tmp = []\n",
    "            for word in sent:\n",
    "                word = self.reduce_length(word)\n",
    "                word = speller(word)\n",
    "                tmp.append(word)\n",
    "\n",
    "            corrected.append(tmp)\n",
    "        \n",
    "        if self.handle_stopwords:\n",
    "            clear_tokens = [[w for w in sent if w not in self.stopwords] \n",
    "                                for sent in corrected]\n",
    "        \n",
    "        return corrected\n",
    "\n",
    "    \n",
    "    def __tokenize(self):\n",
    "        \n",
    "        token_arr = [word_tokenize(tweet) for tweet in self.tweets.tweet]\n",
    "        token_arr = [[w.lower() for w in tokens] for tokens in token_arr]\n",
    "        \n",
    "        if self.handle_stopwords:\n",
    "            token_arr = [[w for w in sent if w not in self.stopwords] for sent in token_arr]\n",
    "            \n",
    "        self.tweets['tokens'] = token_arr \n",
    "        \n",
    "        if self.handle_misspell:\n",
    "            clear_arr = self.misspell_handler(token_arr)\n",
    "            self.tweets['tokens_misspell'] = clear_arr\n",
    "        \n",
    "\n",
    "    def stemming(self):\n",
    "        \n",
    "        ps = PorterStemmer()\n",
    "        stem = [[ps.stem(w) for w in sent] for sent in self.tweets.tokens] \n",
    "        self.tweets['stemming'] = stem\n",
    "        \n",
    "        if self.handle_misspell:\n",
    "            stem_miss = [[ps.stem(w) for w in sent] for sent in self.tweets.tokens_misspell]\n",
    "            self.tweets['stemming_misspell'] = stem_miss\n",
    "\n",
    "        \n",
    "    def lemmatization(self):\n",
    "        \n",
    "        wnl = WordNetLemmatizer()\n",
    "        lemm = [[wnl.lemmatize(w) for w in sent] for sent in self.tweets.tokens]\n",
    "        self.tweets['lemmatization'] = lemm\n",
    "        \n",
    "        if self.handle_misspell:\n",
    "            lemm = [[wnl.lemmatize(w) for w in sent] for sent in self.tweets.tokens_misspell]\n",
    "            self.tweets['lemmatization_misspell'] = lemm\n",
    "\n",
    "        \n",
    "    def transform(self):\n",
    "        \n",
    "        self.stemming()\n",
    "        self.lemmatization()\n",
    "        \n",
    "        empty_rows = self.tweets[(self.tweets.tokens.str.len() == 0)]\n",
    "        self.tweets.drop(empty_rows.index, inplace=True)\n",
    "        self.tweets.reset_index(drop=True, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a878622b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_misspell</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming_misspell</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>lemmatization_misspell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>-1</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where Im going...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, im,...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, im,...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rands likely to fall ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[does, anybody, know, if, the, rands, likely, ...</td>\n",
       "      <td>[does, anybody, know, if, the, hands, likely, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, like, to, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, hand, like, to, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, likely, to...</td>\n",
       "      <td>[doe, anybody, know, if, the, hand, likely, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>-1</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight  unhappy</td>\n",
       "      <td>-1</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, unhappi]</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, unhappi]</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  target  \\\n",
       "0              How unhappy  some dogs like it though      -1   \n",
       "1  talking to my over driver about where Im going...      -1   \n",
       "2  Does anybody know if the Rands likely to fall ...      -1   \n",
       "3         I miss going to gigs in Liverpool unhappy       -1   \n",
       "4       There isnt a new Riverdale tonight  unhappy       -1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0       [how, unhappy, some, dogs, like, it, though]   \n",
       "1  [talking, to, my, over, driver, about, where, ...   \n",
       "2  [does, anybody, know, if, the, rands, likely, ...   \n",
       "3  [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4  [there, isnt, a, new, riverdale, tonight, unha...   \n",
       "\n",
       "                                     tokens_misspell  \\\n",
       "0       [how, unhappy, some, dogs, like, it, though]   \n",
       "1  [talking, to, my, over, driver, about, where, ...   \n",
       "2  [does, anybody, know, if, the, hands, likely, ...   \n",
       "3  [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4  [there, isnt, a, new, riverdale, tonight, unha...   \n",
       "\n",
       "                                            stemming  \\\n",
       "0        [how, unhappi, some, dog, like, it, though]   \n",
       "1  [talk, to, my, over, driver, about, where, im,...   \n",
       "2  [doe, anybodi, know, if, the, rand, like, to, ...   \n",
       "3     [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4  [there, isnt, a, new, riverdal, tonight, unhappi]   \n",
       "\n",
       "                                   stemming_misspell  \\\n",
       "0        [how, unhappi, some, dog, like, it, though]   \n",
       "1  [talk, to, my, over, driver, about, where, im,...   \n",
       "2  [doe, anybodi, know, if, the, hand, like, to, ...   \n",
       "3     [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4  [there, isnt, a, new, riverdal, tonight, unhappi]   \n",
       "\n",
       "                                       lemmatization  \\\n",
       "0        [how, unhappy, some, dog, like, it, though]   \n",
       "1  [talking, to, my, over, driver, about, where, ...   \n",
       "2  [doe, anybody, know, if, the, rand, likely, to...   \n",
       "3  [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4  [there, isnt, a, new, riverdale, tonight, unha...   \n",
       "\n",
       "                              lemmatization_misspell  \n",
       "0        [how, unhappy, some, dog, like, it, though]  \n",
       "1  [talking, to, my, over, driver, about, where, ...  \n",
       "2  [doe, anybody, know, if, the, hand, likely, to...  \n",
       "3  [i, miss, going, to, gig, in, liverpool, unhappy]  \n",
       "4  [there, isnt, a, new, riverdale, tonight, unha...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = Preprocessing(handle_stopwords=False, handle_misspell=True)\n",
    "processed_data.transform()\n",
    "df = processed_data.tweets\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a58d5",
   "metadata": {},
   "source": [
    "### Представление текста\n",
    "\n",
    "Теперь необходиомо привести текст к формату, который можно использовать для обучения модели. Основные способы способы представления текста:\n",
    "\n",
    "\n",
    "**Bag of words (мешок слов)** - в данном представлении текста составляется словарь, где каждому уникальному слову в данных соответствует свой индекс. Отдельное взятое предложение может быть представлено в виде списка, длина которого равна количеству уникальных слов. Если слово встречается в предложении, то в список по индексу этого слова записывается 1, если нет - 0. Так же вместо 1 можно записывать сколько раз слово встречается в предложении.\n",
    "\n",
    "\n",
    "**TF-IDF (Term Frequency, Inverse Document Frequency)** - статистическая мера для оценки важности слов. TF-IDF взвешивает слова в зависимости от частоты их употребления, понижая вес тех слов, которые встречаются слишком часто, что позволяет избавиться от лишних шумов.\n",
    "\n",
    "\n",
    "**Word2vec** - модель, которая обучается на прочтении огромного количества текста с последующим запоминанием того, какое слово возникает в схожих контекстах. После обучения на достаточном количестве данных, Word2Vec генерирует вектор из n измерений для каждого слова в словаре, в котором слова со схожим значением располагаются ближе друг к другу. Так как word2vec генерирует вектор на каждое слово, то для того, чтобы представить предложение в виде вектора, можно посчитать среднее значение векторов, взвешенных по TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67552e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transform:\n",
    "    \n",
    "    \n",
    "    def __init__(self, processed_data):\n",
    "        \n",
    "        self.tweets = processed_data\n",
    "        self.vect_df = pd.DataFrame()\n",
    "        self.vect_df['target'] = processed_data.target\n",
    "        self.vectorizers = {}\n",
    "\n",
    "\n",
    "    def to_bin_vector(self):\n",
    "        '''\n",
    "        Реализация bag of words с бинарным представлением слов\n",
    "        '''\n",
    "        \n",
    "        for col in self.tweets.columns[2:]:\n",
    "            cv = CountVectorizer()\n",
    "            X = cv.fit_transform([' '.join(w) for w in self.tweets[col]])\n",
    "            bin_vec = [[1 if i > 0 else 0 for i in row] for row in X.toarray()]\n",
    "            self.vect_df[f'bin_vect_{col}'] = bin_vec\n",
    "            self.vectorizers[f'bin_vect_{col}'] = cv\n",
    "\n",
    "            \n",
    "    def to_count_vector(self):\n",
    "        '''\n",
    "        Реализация bag of words с учетом количетсва вхождений слов в предложении\n",
    "        '''\n",
    "        \n",
    "        for col in self.tweets.columns[2:]:\n",
    "            cv = CountVectorizer()\n",
    "            X = cv.fit_transform([' '.join(w) for w in self.tweets[col]])\n",
    "            self.vect_df[f'count_vect_{col}'] = list(X.toarray())\n",
    "            self.vectorizers[f'count_vect_{col}'] = cv\n",
    "    \n",
    "    \n",
    "    def to_tfidf(self):\n",
    "        \n",
    "        for col in self.tweets.columns[2:]:\n",
    "            tv = TfidfVectorizer()\n",
    "            X = tv.fit_transform([' '.join(w) for w in self.tweets[col]])\n",
    "            self.vect_df[f'tfidf_vect_{col}'] = list(X.toarray())\n",
    "            self.vectorizers[f'tfidf_vect_{col}'] = tv\n",
    "       \n",
    "    \n",
    "    def word2vec(self, n_dim=50):\n",
    "        \n",
    "        for col in self.tweets.columns[2:]:\n",
    "            res = []\n",
    "            tv = TfidfVectorizer()\n",
    "            tv.fit([' '.join(w) for w in self.tweets[col]])\n",
    "            tf_idf = dict(zip(tv.get_feature_names_out(), list(tv.idf_)))\n",
    "            tf_idf_vocab = tf_idf.keys()\n",
    "            \n",
    "            w2v_model = Word2Vec(self.tweets[col], \n",
    "                                 min_count=1,\n",
    "                                 sg=1,\n",
    "                                 vector_size=n_dim,\n",
    "                                 seed=42)\n",
    "            \n",
    "            mean = np.mean(w2v_model.wv.vectors, 0)\n",
    "            std = np.std(w2v_model.wv.vectors, 0)\n",
    "            for sent in self.tweets[col]:\n",
    "                sent_vec = np.zeros(n_dim)\n",
    "                weight_sum = 0\n",
    "                for word in sent:\n",
    "                    if word in tf_idf_vocab:\n",
    "                        vec = (w2v_model.wv[word] - mean)/std  # нормализуем данные\n",
    "                        weight = tf_idf[word]*(sent.count(word)/len(sent))  # взвешиваем слова по TFIDF\n",
    "                        sent_vec += (vec * weight)\n",
    "                        weight_sum += weight\n",
    "\n",
    "                if weight_sum != 0:\n",
    "                    sent_vec /= weight_sum\n",
    "                res.append(sent_vec)\n",
    "            \n",
    "            self.vect_df[f'w2v_{col}'] = res\n",
    "      \n",
    "    \n",
    "    def glove(self, n_dim=25):\n",
    "        '''\n",
    "        Использование предобученной модели word2vec с расширенным словарем\n",
    "        '''\n",
    "        \n",
    "        glove = gensim.downloader.load(f'glove-twitter-{n_dim}')\n",
    "        \n",
    "        mean = np.mean(glove.vectors, 0)\n",
    "        std = np.std(glove.vectors, 0)\n",
    "        for col in self.tweets.columns[2:]:\n",
    "            res = []\n",
    "            tv = TfidfVectorizer()\n",
    "            tv.fit([' '.join(w) for w in self.tweets[col]])\n",
    "            \n",
    "            tf_idf = dict(zip(tv.get_feature_names_out(), list(tv.idf_)))\n",
    "            glove_vocab = glove.key_to_index.keys()\n",
    "            tf_idf_vocab = tf_idf.keys()\n",
    "            \n",
    "            for sent in self.tweets[col]:\n",
    "                sent_vec = np.zeros(n_dim)\n",
    "                weight_sum = 0\n",
    "                for word in sent:\n",
    "                    if word in tf_idf_vocab and word in glove_vocab:\n",
    "                        vec = (glove[word] - mean)/std\n",
    "                        weight = tf_idf[word]*(sent.count(word)/len(sent))\n",
    "                        sent_vec += (vec * weight)\n",
    "                        weight_sum += weight\n",
    "\n",
    "                if weight_sum != 0:\n",
    "                    sent_vec /= weight_sum\n",
    "                res.append(sent_vec)\n",
    "            \n",
    "            self.vect_df[f'glove_{col}'] = res\n",
    "       \n",
    "    \n",
    "    def apply_all(self):\n",
    "        \n",
    "        self.to_bin_vector()\n",
    "        self.to_count_vector()\n",
    "        self.to_tfidf()\n",
    "        self.word2vec(75)\n",
    "        self.glove(100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a34c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>bin_vect_tokens</th>\n",
       "      <th>bin_vect_tokens_misspell</th>\n",
       "      <th>bin_vect_stemming</th>\n",
       "      <th>bin_vect_stemming_misspell</th>\n",
       "      <th>bin_vect_lemmatization</th>\n",
       "      <th>bin_vect_lemmatization_misspell</th>\n",
       "      <th>count_vect_tokens</th>\n",
       "      <th>count_vect_tokens_misspell</th>\n",
       "      <th>count_vect_stemming</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_stemming</th>\n",
       "      <th>w2v_stemming_misspell</th>\n",
       "      <th>w2v_lemmatization</th>\n",
       "      <th>w2v_lemmatization_misspell</th>\n",
       "      <th>glove_tokens</th>\n",
       "      <th>glove_tokens_misspell</th>\n",
       "      <th>glove_stemming</th>\n",
       "      <th>glove_stemming_misspell</th>\n",
       "      <th>glove_lemmatization</th>\n",
       "      <th>glove_lemmatization_misspell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[2.3242857185004158, -2.4990170738934268, -0.3...</td>\n",
       "      <td>[2.2724655148883124, -1.6505452402067795, -2.6...</td>\n",
       "      <td>[2.4805453225521954, -2.9529819167202866, -3.6...</td>\n",
       "      <td>[2.9564793775604996, -2.2646960576672654, -2.5...</td>\n",
       "      <td>[0.6480318300869508, -0.28423400745306243, 0.4...</td>\n",
       "      <td>[0.6477079265056543, -0.28443119544072054, 0.4...</td>\n",
       "      <td>[0.668538349271827, -0.37048814203797475, 0.37...</td>\n",
       "      <td>[0.668538349271827, -0.37048814203797475, 0.37...</td>\n",
       "      <td>[0.6597960065209195, -0.39388759992503386, 0.4...</td>\n",
       "      <td>[0.6594731282160216, -0.39418283079931055, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.994291325033083, -2.112010639718187, 0.4588...</td>\n",
       "      <td>[1.943228230612658, -1.5144923048767946, -0.76...</td>\n",
       "      <td>[2.112622632700753, -2.60275328570693, -2.9924...</td>\n",
       "      <td>[2.3683792207979226, -2.025272705950721, -2.20...</td>\n",
       "      <td>[0.7212703938964621, 0.33395460218286865, 0.46...</td>\n",
       "      <td>[0.6983579820038477, 0.3414635418031642, 0.484...</td>\n",
       "      <td>[0.5486683360131586, 0.3100470660555184, 0.319...</td>\n",
       "      <td>[0.49713198004630604, 0.355012953494765, 0.358...</td>\n",
       "      <td>[0.7252669653610138, 0.355449039481583, 0.4733...</td>\n",
       "      <td>[0.7019244034324438, 0.362309880220061, 0.4928...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.8003574356036465, -1.9610387400586475, 0.43...</td>\n",
       "      <td>[1.8454155314364025, -1.3068454754857273, -0.6...</td>\n",
       "      <td>[1.7157942045225962, -2.1822666145892367, -2.5...</td>\n",
       "      <td>[2.102266354520098, -1.7090745285088509, -1.58...</td>\n",
       "      <td>[0.28574667106518564, 0.27927008990920615, 0.2...</td>\n",
       "      <td>[0.35918648362999817, 0.2442847161534092, 0.24...</td>\n",
       "      <td>[0.12973172580630823, 0.30124781782919535, 0.2...</td>\n",
       "      <td>[0.17398689734716866, 0.2175394253294878, 0.14...</td>\n",
       "      <td>[0.29739961492881417, 0.3633682726187642, 0.19...</td>\n",
       "      <td>[0.3386214671475524, 0.2891211149288993, 0.095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.280970317055139, -0.9460966515757131, 0.902...</td>\n",
       "      <td>[1.2241550658179137, -0.6973038940718105, -1.3...</td>\n",
       "      <td>[1.5110283494642982, -1.6273415476315318, -1.5...</td>\n",
       "      <td>[1.6960533388038481, -1.2555043362274299, -0.7...</td>\n",
       "      <td>[0.22231244385910687, 0.18492170189069537, 0.7...</td>\n",
       "      <td>[0.22237786522955177, 0.1846918876149743, 0.75...</td>\n",
       "      <td>[0.09449381668260298, 0.7801656517226432, 0.68...</td>\n",
       "      <td>[0.09453320168555919, 0.7801679991210608, 0.68...</td>\n",
       "      <td>[0.14475583485912283, 0.633365361609054, 0.797...</td>\n",
       "      <td>[0.14469404397955868, 0.633871107173953, 0.797...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.750638663588911, -2.024681282886285, 0.1327...</td>\n",
       "      <td>[1.5228035621864235, -1.0602074804298587, -1.0...</td>\n",
       "      <td>[1.9366248529719352, -2.2393622787898337, -2.6...</td>\n",
       "      <td>[2.0471627606583387, -1.9188825043107416, -1.8...</td>\n",
       "      <td>[0.18369200287731746, 0.38293295545078954, 0.3...</td>\n",
       "      <td>[0.18461196721889186, 0.3803651248519063, 0.32...</td>\n",
       "      <td>[0.1961412042471419, 0.49526036788130484, 0.07...</td>\n",
       "      <td>[0.19557643709193806, 0.4947385287572735, 0.07...</td>\n",
       "      <td>[0.18457289447553116, 0.3798479381446972, 0.32...</td>\n",
       "      <td>[0.18405008378445853, 0.37950905332718216, 0.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                    bin_vect_tokens  \\\n",
       "0      -1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      -1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      -1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      -1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      -1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            bin_vect_tokens_misspell  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                   bin_vect_stemming  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                          bin_vect_stemming_misspell  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                              bin_vect_lemmatization  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                     bin_vect_lemmatization_misspell  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                   count_vect_tokens  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                          count_vect_tokens_misspell  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                 count_vect_stemming  ...  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...   \n",
       "\n",
       "                                        w2v_stemming  \\\n",
       "0  [2.3242857185004158, -2.4990170738934268, -0.3...   \n",
       "1  [1.994291325033083, -2.112010639718187, 0.4588...   \n",
       "2  [1.8003574356036465, -1.9610387400586475, 0.43...   \n",
       "3  [1.280970317055139, -0.9460966515757131, 0.902...   \n",
       "4  [1.750638663588911, -2.024681282886285, 0.1327...   \n",
       "\n",
       "                               w2v_stemming_misspell  \\\n",
       "0  [2.2724655148883124, -1.6505452402067795, -2.6...   \n",
       "1  [1.943228230612658, -1.5144923048767946, -0.76...   \n",
       "2  [1.8454155314364025, -1.3068454754857273, -0.6...   \n",
       "3  [1.2241550658179137, -0.6973038940718105, -1.3...   \n",
       "4  [1.5228035621864235, -1.0602074804298587, -1.0...   \n",
       "\n",
       "                                   w2v_lemmatization  \\\n",
       "0  [2.4805453225521954, -2.9529819167202866, -3.6...   \n",
       "1  [2.112622632700753, -2.60275328570693, -2.9924...   \n",
       "2  [1.7157942045225962, -2.1822666145892367, -2.5...   \n",
       "3  [1.5110283494642982, -1.6273415476315318, -1.5...   \n",
       "4  [1.9366248529719352, -2.2393622787898337, -2.6...   \n",
       "\n",
       "                          w2v_lemmatization_misspell  \\\n",
       "0  [2.9564793775604996, -2.2646960576672654, -2.5...   \n",
       "1  [2.3683792207979226, -2.025272705950721, -2.20...   \n",
       "2  [2.102266354520098, -1.7090745285088509, -1.58...   \n",
       "3  [1.6960533388038481, -1.2555043362274299, -0.7...   \n",
       "4  [2.0471627606583387, -1.9188825043107416, -1.8...   \n",
       "\n",
       "                                        glove_tokens  \\\n",
       "0  [0.6480318300869508, -0.28423400745306243, 0.4...   \n",
       "1  [0.7212703938964621, 0.33395460218286865, 0.46...   \n",
       "2  [0.28574667106518564, 0.27927008990920615, 0.2...   \n",
       "3  [0.22231244385910687, 0.18492170189069537, 0.7...   \n",
       "4  [0.18369200287731746, 0.38293295545078954, 0.3...   \n",
       "\n",
       "                               glove_tokens_misspell  \\\n",
       "0  [0.6477079265056543, -0.28443119544072054, 0.4...   \n",
       "1  [0.6983579820038477, 0.3414635418031642, 0.484...   \n",
       "2  [0.35918648362999817, 0.2442847161534092, 0.24...   \n",
       "3  [0.22237786522955177, 0.1846918876149743, 0.75...   \n",
       "4  [0.18461196721889186, 0.3803651248519063, 0.32...   \n",
       "\n",
       "                                      glove_stemming  \\\n",
       "0  [0.668538349271827, -0.37048814203797475, 0.37...   \n",
       "1  [0.5486683360131586, 0.3100470660555184, 0.319...   \n",
       "2  [0.12973172580630823, 0.30124781782919535, 0.2...   \n",
       "3  [0.09449381668260298, 0.7801656517226432, 0.68...   \n",
       "4  [0.1961412042471419, 0.49526036788130484, 0.07...   \n",
       "\n",
       "                             glove_stemming_misspell  \\\n",
       "0  [0.668538349271827, -0.37048814203797475, 0.37...   \n",
       "1  [0.49713198004630604, 0.355012953494765, 0.358...   \n",
       "2  [0.17398689734716866, 0.2175394253294878, 0.14...   \n",
       "3  [0.09453320168555919, 0.7801679991210608, 0.68...   \n",
       "4  [0.19557643709193806, 0.4947385287572735, 0.07...   \n",
       "\n",
       "                                 glove_lemmatization  \\\n",
       "0  [0.6597960065209195, -0.39388759992503386, 0.4...   \n",
       "1  [0.7252669653610138, 0.355449039481583, 0.4733...   \n",
       "2  [0.29739961492881417, 0.3633682726187642, 0.19...   \n",
       "3  [0.14475583485912283, 0.633365361609054, 0.797...   \n",
       "4  [0.18457289447553116, 0.3798479381446972, 0.32...   \n",
       "\n",
       "                        glove_lemmatization_misspell  \n",
       "0  [0.6594731282160216, -0.39418283079931055, 0.4...  \n",
       "1  [0.7019244034324438, 0.362309880220061, 0.4928...  \n",
       "2  [0.3386214671475524, 0.2891211149288993, 0.095...  \n",
       "3  [0.14469404397955868, 0.633871107173953, 0.797...  \n",
       "4  [0.18405008378445853, 0.37950905332718216, 0.3...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = transform(df)\n",
    "vectors.apply_all()\n",
    "vectors.vect_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd21ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_models(model_arr, vect_df):\n",
    "    \n",
    "#     result = pd.DataFrame(index=vect_df.columns[1:])\n",
    "    \n",
    "#     for model in model_arr:\n",
    "#         scores = []\n",
    "#         for col in vectors[1:]:\n",
    "#             X = vectors[col].to_list()\n",
    "#             y = vectors.target.to_list()\n",
    "\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                                train_size=0.8,\n",
    "#                                                                random_state=42,\n",
    "#                                                                shuffle=True)\n",
    "\n",
    "#             model.fit(X_train, y_train)\n",
    "#             y_pred = model.predict(X_test)\n",
    "#             accuracy = accuracy_score(y_pred, y_test)\n",
    "#             scores.append(accuracy)\n",
    "\n",
    "#         result[f'{model}_accuracy'] = scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751f5f5",
   "metadata": {},
   "source": [
    "## Обучение моделей\n",
    "\n",
    "Теперь будем обучать модели с помощью различных подходов к обработке и представлению данных и сравним результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cb37e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train():\n",
    "    \n",
    "    \n",
    "    def __init__(self, vect_df):\n",
    "        \n",
    "        self.vect_df = vect_df\n",
    "        self.models = {}\n",
    "        \n",
    "        \n",
    "    def LogRegress(self):\n",
    "        \n",
    "        lr = LogisticRegression(multi_class='multinomial',\n",
    "                                solver='lbfgs', max_iter=1000)\n",
    "        self.models['LogRegress'] = lr\n",
    "       \n",
    "    \n",
    "    def SVC(self):\n",
    "        \n",
    "        svc = svm.SVC()\n",
    "        self.models['SVC'] = svc\n",
    "    \n",
    "    \n",
    "    def GBM(self):\n",
    "        \n",
    "        gbm = GradientBoostingClassifier()\n",
    "        self.models['GBM'] = gbm\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        \n",
    "        self.LogRegress()\n",
    "        self.SVC()\n",
    "        self.GBM()\n",
    "        \n",
    "        result = pd.DataFrame(index=self.vect_df.columns[1:])\n",
    "    \n",
    "        for name, model in self.models.items():\n",
    "            scores = []\n",
    "            for col in self.vect_df.columns[1:]:\n",
    "                X = self.vect_df[col].to_list()\n",
    "                y = self.vect_df.target.to_list()\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                                   train_size=0.8,\n",
    "                                                                   random_state=42,\n",
    "                                                                   shuffle=True)\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                accuracy = accuracy_score(y_pred, y_test)\n",
    "                scores.append(accuracy)\n",
    "\n",
    "            result[f'{name}_accuracy'] = scores\n",
    "            \n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44580748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogRegress_accuracy</th>\n",
       "      <th>SVC_accuracy</th>\n",
       "      <th>GBM_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bin_vect_tokens</th>\n",
       "      <td>0.922509</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_vect_tokens_misspell</th>\n",
       "      <td>0.920664</td>\n",
       "      <td>0.928044</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_vect_stemming</th>\n",
       "      <td>0.931734</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_vect_stemming_misspell</th>\n",
       "      <td>0.931734</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_vect_lemmatization</th>\n",
       "      <td>0.926199</td>\n",
       "      <td>0.928044</td>\n",
       "      <td>0.915129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_vect_lemmatization_misspell</th>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.929889</td>\n",
       "      <td>0.915129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_vect_tokens</th>\n",
       "      <td>0.918819</td>\n",
       "      <td>0.922509</td>\n",
       "      <td>0.913284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_vect_tokens_misspell</th>\n",
       "      <td>0.916974</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.918819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_vect_stemming</th>\n",
       "      <td>0.926199</td>\n",
       "      <td>0.928044</td>\n",
       "      <td>0.913284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_vect_stemming_misspell</th>\n",
       "      <td>0.916974</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_vect_lemmatization</th>\n",
       "      <td>0.920664</td>\n",
       "      <td>0.928044</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_vect_lemmatization_misspell</th>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.928044</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_vect_tokens</th>\n",
       "      <td>0.915129</td>\n",
       "      <td>0.915129</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_vect_tokens_misspell</th>\n",
       "      <td>0.918819</td>\n",
       "      <td>0.922509</td>\n",
       "      <td>0.916974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_vect_stemming</th>\n",
       "      <td>0.915129</td>\n",
       "      <td>0.922509</td>\n",
       "      <td>0.915129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_vect_stemming_misspell</th>\n",
       "      <td>0.913284</td>\n",
       "      <td>0.928044</td>\n",
       "      <td>0.915129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_vect_lemmatization</th>\n",
       "      <td>0.913284</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.915129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_vect_lemmatization_misspell</th>\n",
       "      <td>0.915129</td>\n",
       "      <td>0.920664</td>\n",
       "      <td>0.924354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_tokens</th>\n",
       "      <td>0.826568</td>\n",
       "      <td>0.789668</td>\n",
       "      <td>0.773063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_tokens_misspell</th>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.782288</td>\n",
       "      <td>0.773063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_stemming</th>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.789668</td>\n",
       "      <td>0.804428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_stemming_misspell</th>\n",
       "      <td>0.813653</td>\n",
       "      <td>0.798893</td>\n",
       "      <td>0.774908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_lemmatization</th>\n",
       "      <td>0.817343</td>\n",
       "      <td>0.765683</td>\n",
       "      <td>0.760148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v_lemmatization_misspell</th>\n",
       "      <td>0.845018</td>\n",
       "      <td>0.784133</td>\n",
       "      <td>0.771218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove_tokens</th>\n",
       "      <td>0.848708</td>\n",
       "      <td>0.841328</td>\n",
       "      <td>0.830258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove_tokens_misspell</th>\n",
       "      <td>0.824723</td>\n",
       "      <td>0.841328</td>\n",
       "      <td>0.822878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove_stemming</th>\n",
       "      <td>0.778598</td>\n",
       "      <td>0.795203</td>\n",
       "      <td>0.798893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove_stemming_misspell</th>\n",
       "      <td>0.769373</td>\n",
       "      <td>0.776753</td>\n",
       "      <td>0.780443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove_lemmatization</th>\n",
       "      <td>0.841328</td>\n",
       "      <td>0.843173</td>\n",
       "      <td>0.835793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove_lemmatization_misspell</th>\n",
       "      <td>0.832103</td>\n",
       "      <td>0.843173</td>\n",
       "      <td>0.815498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   LogRegress_accuracy  SVC_accuracy  \\\n",
       "bin_vect_tokens                               0.922509      0.926199   \n",
       "bin_vect_tokens_misspell                      0.920664      0.928044   \n",
       "bin_vect_stemming                             0.931734      0.926199   \n",
       "bin_vect_stemming_misspell                    0.931734      0.924354   \n",
       "bin_vect_lemmatization                        0.926199      0.928044   \n",
       "bin_vect_lemmatization_misspell               0.924354      0.929889   \n",
       "count_vect_tokens                             0.918819      0.922509   \n",
       "count_vect_tokens_misspell                    0.916974      0.924354   \n",
       "count_vect_stemming                           0.926199      0.928044   \n",
       "count_vect_stemming_misspell                  0.916974      0.926199   \n",
       "count_vect_lemmatization                      0.920664      0.928044   \n",
       "count_vect_lemmatization_misspell             0.924354      0.928044   \n",
       "tfidf_vect_tokens                             0.915129      0.915129   \n",
       "tfidf_vect_tokens_misspell                    0.918819      0.922509   \n",
       "tfidf_vect_stemming                           0.915129      0.922509   \n",
       "tfidf_vect_stemming_misspell                  0.913284      0.928044   \n",
       "tfidf_vect_lemmatization                      0.913284      0.924354   \n",
       "tfidf_vect_lemmatization_misspell             0.915129      0.920664   \n",
       "w2v_tokens                                    0.826568      0.789668   \n",
       "w2v_tokens_misspell                           0.811808      0.782288   \n",
       "w2v_stemming                                  0.821033      0.789668   \n",
       "w2v_stemming_misspell                         0.813653      0.798893   \n",
       "w2v_lemmatization                             0.817343      0.765683   \n",
       "w2v_lemmatization_misspell                    0.845018      0.784133   \n",
       "glove_tokens                                  0.848708      0.841328   \n",
       "glove_tokens_misspell                         0.824723      0.841328   \n",
       "glove_stemming                                0.778598      0.795203   \n",
       "glove_stemming_misspell                       0.769373      0.776753   \n",
       "glove_lemmatization                           0.841328      0.843173   \n",
       "glove_lemmatization_misspell                  0.832103      0.843173   \n",
       "\n",
       "                                   GBM_accuracy  \n",
       "bin_vect_tokens                        0.916974  \n",
       "bin_vect_tokens_misspell               0.916974  \n",
       "bin_vect_stemming                      0.916974  \n",
       "bin_vect_stemming_misspell             0.916974  \n",
       "bin_vect_lemmatization                 0.915129  \n",
       "bin_vect_lemmatization_misspell        0.915129  \n",
       "count_vect_tokens                      0.913284  \n",
       "count_vect_tokens_misspell             0.918819  \n",
       "count_vect_stemming                    0.913284  \n",
       "count_vect_stemming_misspell           0.916974  \n",
       "count_vect_lemmatization               0.916974  \n",
       "count_vect_lemmatization_misspell      0.916974  \n",
       "tfidf_vect_tokens                      0.916974  \n",
       "tfidf_vect_tokens_misspell             0.916974  \n",
       "tfidf_vect_stemming                    0.915129  \n",
       "tfidf_vect_stemming_misspell           0.915129  \n",
       "tfidf_vect_lemmatization               0.915129  \n",
       "tfidf_vect_lemmatization_misspell      0.924354  \n",
       "w2v_tokens                             0.773063  \n",
       "w2v_tokens_misspell                    0.773063  \n",
       "w2v_stemming                           0.804428  \n",
       "w2v_stemming_misspell                  0.774908  \n",
       "w2v_lemmatization                      0.760148  \n",
       "w2v_lemmatization_misspell             0.771218  \n",
       "glove_tokens                           0.830258  \n",
       "glove_tokens_misspell                  0.822878  \n",
       "glove_stemming                         0.798893  \n",
       "glove_stemming_misspell                0.780443  \n",
       "glove_lemmatization                    0.835793  \n",
       "glove_lemmatization_misspell           0.815498  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_df = vectors.vect_df\n",
    "models = Train(vect_df)\n",
    "final_res = models.test()\n",
    "final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71552d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63ed074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogRegress_accuracy    0.880074\n",
       "SVC_accuracy           0.876814\n",
       "GBM_accuracy           0.867958\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e2384e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogRegress_accuracy    0.931734\n",
       "SVC_accuracy           0.929889\n",
       "GBM_accuracy           0.924354\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c4228c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogRegress_accuracy                    bin_vect_stemming\n",
       "SVC_accuracy             bin_vect_lemmatization_misspell\n",
       "GBM_accuracy           tfidf_vect_lemmatization_misspell\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66db266",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "Линейные модели дали лучший результат в поставленой задаче. Высокая точность была достигнута с использованием обычного мешка слов и с сохранением стоп слов. \n",
    "\n",
    "Среди подходов к приведению слов к общей форме лучше всего себя продемонстрировала лемматизация. При этом точность word2vec при использовании лемматизации значительно растет, т.к. word2vec пытается определить именно смысловое значение слов.\n",
    "\n",
    "Также, стоит пояснить один важный момент: в данной задаче необходимо правильно выявить не только негативные/позитивные твиты, но и нейтральные. Таким образом, сохранив стоп-слова, мы наблюдаем повышение точности модели, т.к. нейтральные твиты содержат в себе много \"шумовых\" слов. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b159824",
   "metadata": {},
   "source": [
    "## Using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b55eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self, vect_df, vectors):\n",
    "        \n",
    "        lr = LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000\n",
    "        )\n",
    "        \n",
    "        self.vectorizer = vectors.vectorizers['bin_vect_stemming']\n",
    "        \n",
    "        X = vect_df['bin_vect_stemming'].to_list()\n",
    "        y = vect_df['target'].to_list()\n",
    "        \n",
    "        lr.fit(X, y)\n",
    "        self.lr = lr\n",
    "\n",
    "        \n",
    "    def classify(self, text):\n",
    "        \n",
    "        ps = PorterStemmer()\n",
    "        tokens = [word_tokenize(sent) for sent in text]\n",
    "        processed_text = [[ps.stem(w) for w in sent] for sent in tokens]\n",
    "        processed_text = [' '.join(w) for w in processed_text]\n",
    "        \n",
    "        vectors = self.vectorizer.transform(processed_text)\n",
    "        vectors = list(vectors.toarray())\n",
    "        pred = self.lr.predict(vectors)\n",
    "        \n",
    "        res = pd.DataFrame()\n",
    "        res['tweets'] = text\n",
    "        res['prediction'] = pred\n",
    "        \n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad60e8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my dear friend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>so happy for you</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love you baby</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jesus christ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>such a shame</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boring</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i feel very lonely and unhappy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>so sad</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           tweets  prediction\n",
       "0                  my dear friend           1\n",
       "1                so happy for you           1\n",
       "2                   love you baby           1\n",
       "3                    jesus christ           0\n",
       "4                    such a shame           0\n",
       "5                          boring           0\n",
       "6  i feel very lonely and unhappy          -1\n",
       "7                          so sad          -1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    'my dear friend',\n",
    "    'so happy for you',\n",
    "    'love you baby',\n",
    "    'jesus christ',\n",
    "    'such a shame',\n",
    "    'boring',\n",
    "    'i feel very lonely and unhappy',\n",
    "    'so sad'\n",
    "]\n",
    "\n",
    "tc = TextClassifier(vect_df, vectors)\n",
    "tc.classify(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
